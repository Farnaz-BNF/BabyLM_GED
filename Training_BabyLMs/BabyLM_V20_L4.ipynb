{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajNr4mE7_n5Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "from transformers.trainer import *\n",
        "from transformers import RobertaConfig\n",
        "from transformers import RobertaForMaskedLM\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import RobertaTokenizerFast\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import AutoTokenizer, RobertaModel\n",
        "from transformers import DataCollatorForLanguageModeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89NPyPRbUlPx",
        "outputId": "dd53a4f6-4266-4396-90ea-f88b8b48cc53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyeTVVi-WP2n"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtfvzGt5EEVa"
      },
      "outputs": [],
      "source": [
        "def load_data(data_directory_path):\n",
        "  all_data = []\n",
        "  total_sentence = 0\n",
        "  for filename in os.listdir(data_directory_path):\n",
        "    file_path = os.path.join(data_directory_path, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "      print(f'Reading file: {filename}')\n",
        "      with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "          total_sentence+=1\n",
        "          all_data.append(line.strip())\n",
        "  print(f\"total sentece in directory: {total_sentence}\")\n",
        "  return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAYA9cYv_pM0",
        "outputId": "3af343c7-be46-4be3-e8d5-1b4c6ab11691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading file: wikipedia.train\n",
            "Reading file: gutenberg.train\n",
            "Reading file: open_subtitles.train\n",
            "Reading file: simple_wikipedia.train\n",
            "Reading file: qed.train\n",
            "Reading file: aochildes.train\n",
            "Reading file: bnc_spoken.train\n",
            "Reading file: switchboard.train\n",
            "Reading file: children_stories.train\n",
            "Reading file: cbt.train\n",
            "total sentece in directory: 1058740\n",
            "==================================================\n",
            "Reading file: qed.dev\n",
            "Reading file: bnc_spoken.dev\n",
            "Reading file: cbt.dev\n",
            "Reading file: children_stories.dev\n",
            "Reading file: gutenberg.dev\n",
            "Reading file: aochildes.dev\n",
            "Reading file: open_subtitles.dev\n",
            "Reading file: simple_wikipedia.dev\n",
            "Reading file: wikipedia.dev\n",
            "Reading file: switchboard.dev\n",
            "total sentece in directory: 1026747\n",
            "==================================================\n",
            "Reading file: children_stories.test\n",
            "Reading file: cbt.test\n",
            "Reading file: aochildes.test\n",
            "Reading file: gutenberg.test\n",
            "Reading file: bnc_spoken.test\n",
            "Reading file: wikipedia.test\n",
            "Reading file: simple_wikipedia.test\n",
            "Reading file: open_subtitles.test\n",
            "Reading file: switchboard.test\n",
            "Reading file: qed.test\n",
            "total sentece in directory: 1054646\n",
            "==================================================\n",
            "1058740\n",
            "1026747\n",
            "1054646\n"
          ]
        }
      ],
      "source": [
        "train_directory_path = '/content/drive/MyDrive/VU Thesis/Code/Data/babylm_10M'\n",
        "sequences = load_data(train_directory_path)\n",
        "print('='*50)\n",
        "\n",
        "dev_directory_path = '/content/drive/MyDrive/VU Thesis/Code/Data/babylm_dev'\n",
        "devset = load_data(dev_directory_path)\n",
        "print('='*50)\n",
        "\n",
        "test_directory_path = '/content/drive/MyDrive/VU Thesis/Code/Data/babylm_test'\n",
        "test_data = load_data(test_directory_path)\n",
        "print('='*50)\n",
        "\n",
        "print(len(sequences))\n",
        "print(len(devset))\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzNl_-6P_qif"
      },
      "outputs": [],
      "source": [
        "tokenizer_folder = '/content/drive/MyDrive/VU Thesis/Code/baby_models/tokenizer_V20_folder'\n",
        "\n",
        "# if not os.path.exists(tokenizer_folder):\n",
        "  # os.mkdir(tokenizer_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A_EdNBWGjNP"
      },
      "outputs": [],
      "source": [
        "def train_tokenizer(sequence_data, vocab_size, tokenizer_folder):\n",
        "  # Initialize tokenizer\n",
        "  tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "  tokenizer.train_from_iterator(sequences,\n",
        "                                vocab_size=vocab_size,\n",
        "                                min_frequency=2,\n",
        "                                show_progress=True,\n",
        "                                special_tokens=[\n",
        "                                    \"<s>\",\n",
        "                                    \"<pad>\",\n",
        "                                    \"</s>\",\n",
        "                                    \"<unk>\",\n",
        "                                    \"<mask>\"]\n",
        "                                )\n",
        "  # Save tokenizer\n",
        "  tokenizer.save_model(tokenizer_folder)\n",
        "\n",
        "if not os.path.exists(tokenizer_folder):\n",
        "  os.mkdir(tokenizer_folder)\n",
        "  train_tokenizer(sequences, 20_480, tokenizer_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8-vw_zrqBhX"
      },
      "outputs": [],
      "source": [
        "# create CustomDataset class\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.examples = []\n",
        "    self.mask = []\n",
        "    max_length = 512\n",
        "    for example in data:\n",
        "      x=tokenizer.encode_plus(example, max_length = max_length, truncation=True, padding=True)\n",
        "      self.examples += [x.input_ids]\n",
        "      self.mask += [x.attention_mask]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return torch.tensor(self.examples[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyWPGTuJqIB0"
      },
      "outputs": [],
      "source": [
        "def get_train_dataloader(self) -> DataLoader:\n",
        "    \"\"\"\n",
        "    Returns the training :class:`~torch.utils.data.DataLoader`.\n",
        "\n",
        "    Will use no sampler if :obj:`self.train_dataset` does not implement :obj:`__len__`, a random sampler (adapted\n",
        "    to distributed training if necessary) otherwise.\n",
        "\n",
        "    Subclass and override this method if you want to inject some custom behavior.\n",
        "    \"\"\"\n",
        "    if self.train_dataset is None:\n",
        "        raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
        "\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.args.train_batch_size,\n",
        "        sampler=SequentialSampler(self.train_dataset),\n",
        "        collate_fn=self.data_collator,\n",
        "        drop_last=self.args.dataloader_drop_last,\n",
        "        num_workers=self.args.dataloader_num_workers,\n",
        "        shuffle=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clxn9BJZpKoS"
      },
      "outputs": [],
      "source": [
        "def train_baby_lm(babaylm_config, tokenizer_path, train_data, dev_data, epochs_start, epochs_number, batch_size):\n",
        "\n",
        "    max_length = 512\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path, max_len=max_length)\n",
        "\n",
        "    # Define data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "    Trainer.get_train_dataloader = get_train_dataloader\n",
        "\n",
        "    for i in range(epochs_start, epochs_start + epochs_number):\n",
        "\n",
        "        if os.path.exists(f'{model_path}/model_e{i}_v20_l4'):\n",
        "            model = RobertaForMaskedLM.from_pretrained(f'{model_path}/model_e{i}_v20_l4')\n",
        "            print(f\"pretrained model load (from Epoch {i})...\")\n",
        "\n",
        "        else:\n",
        "            print(\"No checkpoint ... Create New one\")\n",
        "            model = RobertaForMaskedLM(config=babaylm_config)\n",
        "            print('Model parameters: ',model.num_parameters())\n",
        "\n",
        "        # batch_size = 16\n",
        "        # Define training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f'{model_path}/run_model_{i+1}_folder',\n",
        "            overwrite_output_dir=True,\n",
        "            eval_strategy = 'epoch',\n",
        "            num_train_epochs=1,\n",
        "            learning_rate=1e-4,\n",
        "            weight_decay=0.01,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            save_strategy=\"epoch\",\n",
        "            # save_steps=8192,\n",
        "            save_total_limit=1,\n",
        "            max_steps=int(len(train_dataset)/ batch_size)\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_data,#train_dataset,\n",
        "            eval_dataset=dev_data,#eval_dataset,\n",
        "            tokenizer=tokenizer)\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.save_model(f'{model_path}/model_e{i+1}_v20_l4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KFanF-9wUr1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "max_length = 512\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=max_length)\n",
        "\n",
        "train_custom_data_path =  \"/content/drive/MyDrive/VU Thesis/Code/baby_models/train_V20.pkl\"\n",
        "dev_custom_data_path = \"/content/drive/MyDrive/VU Thesis/Code/baby_models/dev_V20.pkl\"\n",
        "\n",
        "if os.path.isfile(train_custom_data_path):\n",
        "  with open(train_custom_data_path, 'rb') as file:\n",
        "    train_dataset = pickle.load(file)\n",
        "else:\n",
        "  train_dataset = CustomDataset(sequences, tokenizer)\n",
        "  with open(train_custom_data_path, 'wb') as file:\n",
        "    pickle.dump(train_dataset, file)\n",
        "\n",
        "\n",
        "if os.path.isfile(dev_custom_data_path):\n",
        "  with open(dev_custom_data_path, 'rb') as file:\n",
        "    eval_dataset = pickle.load(file)\n",
        "else:\n",
        "  eval_dataset = CustomDataset(devset, tokenizer)\n",
        "  with open(dev_custom_data_path, 'wb') as file:\n",
        "    pickle.dump(eval_dataset, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CxgrV8m0sxmW",
        "outputId": "c4782ca0-ab33-4782-e34f-6ab145c05c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters:  12823552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-10-50f2f0b75d44>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfarnaz-banifatemi-vu\u001b[0m (\u001b[33mfarnaz-banifatemi-vu-vrije-universiteit-amsterdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250413_225906-i77ya950</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface/runs/i77ya950' target=\"_blank\">/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/run_model_1_folder</a></strong> to <a href='https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface' target=\"_blank\">https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface/runs/i77ya950' target=\"_blank\">https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface/runs/i77ya950</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 36:40, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.241600</td>\n",
              "      <td>5.002483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-10-50f2f0b75d44>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 37:22, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.597800</td>\n",
              "      <td>4.494562</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-10-50f2f0b75d44>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 37:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.373400</td>\n",
              "      <td>4.294209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-10-50f2f0b75d44>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 38:25, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.281600</td>\n",
              "      <td>4.192483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-10-50f2f0b75d44>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 38:16, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.234500</td>\n",
              "      <td>4.136362</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Epochs 1-5\n",
        "config = RobertaConfig(\n",
        "    vocab_size=20480,\n",
        "    num_hidden_layers=4,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=8,\n",
        "    type_vocab_size=1,\n",
        "    intermediate_size=3072,\n",
        "    layer_norm_eps = 1e-05,\n",
        "    hidden_size=256,\n",
        "    initializer_range=0.02,\n",
        "    classifier_dropout = None,\n",
        "    hidden_act=\"gelu\",\n",
        "    position_embedding_type= \"absolute\",\n",
        "    hidden_dropout_prob= 0.1,\n",
        "    attention_probs_dropout_prob= 0.1,\n",
        ")\n",
        "\n",
        "epochs_start = 0\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "train_baby_lm(config, tokenizer_folder, train_dataset, eval_dataset, epochs_start, epochs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ivTOA_wR6X3v",
        "outputId": "a21cf4f7-ae9b-4cab-da70-6c0dd6daec2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pretrained model load (from Epoch 5)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-57c649510391>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfarnaz-banifatemi-vu\u001b[0m (\u001b[33mfarnaz-banifatemi-vu-vrije-universiteit-amsterdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250415_091601-n2o7vwpz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface/runs/n2o7vwpz' target=\"_blank\">/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/run_model_6_folder</a></strong> to <a href='https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface' target=\"_blank\">https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface/runs/n2o7vwpz' target=\"_blank\">https://wandb.ai/farnaz-banifatemi-vu-vrije-universiteit-amsterdam/huggingface/runs/n2o7vwpz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 36:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.214000</td>\n",
              "      <td>4.115608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pretrained model load (from Epoch 6)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-57c649510391>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 35:52, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.214500</td>\n",
              "      <td>4.115007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pretrained model load (from Epoch 7)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-57c649510391>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 35:57, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.232900</td>\n",
              "      <td>4.122503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pretrained model load (from Epoch 8)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-57c649510391>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 35:50, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.258800</td>\n",
              "      <td>4.135951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pretrained model load (from Epoch 9)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-57c649510391>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33085' max='33085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33085/33085 36:19, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.291700</td>\n",
              "      <td>4.155323</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Epochs 6-10\n",
        "config = RobertaConfig(\n",
        "    vocab_size=20480,\n",
        "    num_hidden_layers=4,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=8,\n",
        "    type_vocab_size=1,\n",
        "    intermediate_size=3072,\n",
        "    layer_norm_eps = 1e-05,\n",
        "    hidden_size=256,\n",
        "    initializer_range=0.02,\n",
        "    classifier_dropout = None,\n",
        "    hidden_act=\"gelu\",\n",
        "    position_embedding_type= \"absolute\",\n",
        "    hidden_dropout_prob= 0.1,\n",
        "    attention_probs_dropout_prob= 0.1,\n",
        ")\n",
        "\n",
        "epochs_start = 5\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "train_baby_lm(config, tokenizer_folder, train_dataset, eval_dataset, epochs_start, epochs, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0J3NNwdac3C"
      },
      "source": [
        "## BLiMP Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPR0rs54ageo"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/VU Thesis/Code/BliMP_eval_code/evaluation-pipeline\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s18aZ9f0btoZ"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/VU Thesis/Code/BliMP_eval_code/lm_eval\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EdBLVsfbnbu"
      },
      "outputs": [],
      "source": [
        "!mkdir model_folder\n",
        "!cp \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4/config.json\" \"/content/model_folder\"\n",
        "!cp \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4/merges.txt\" \"/content/model_folder\"\n",
        "!cp \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4/model.safetensors\" \"/content/model_folder\"\n",
        "!cp \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4/special_tokens_map.json\" \"/content/model_folder\"\n",
        "!cp \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4/tokenizer.json\" \"/content/model_folder\"\n",
        "!cp \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4/tokenizer_config.json\" \"/content/model_folder\"\n",
        "!cp \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4/training_args.bin\" \"/content/model_folder\"\n",
        "!cp \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4/vocab.json\" \"/content/model_folder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VcKAHtFpbwgb",
        "outputId": "c4a1c073-8ac8-4162-e0ef-8b3614599a68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping lm-eval as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining file:///content/evaluation-pipeline\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate@ git+https://github.com/huggingface/accelerate@main (from lm_eval==0.2.0)\n",
            "  Cloning https://github.com/huggingface/accelerate (to revision main) to /tmp/pip-install-ez2vrbuq/accelerate_8737c2bcd6e24725950259f404a703f7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-install-ez2vrbuq/accelerate_8737c2bcd6e24725950259f404a703f7\n",
            "  Resolved https://github.com/huggingface/accelerate to commit 34c1779828b3d0769992e6492e6de93d869f71b5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets>=2.0.0 (from lm_eval==0.2.0)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting nltk==3.6 (from lm_eval==0.2.0)\n",
            "  Downloading nltk-3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting openai==0.13.0 (from lm_eval==0.2.0)\n",
            "  Downloading openai-0.13.0.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycountry==20.7.3 (from lm_eval==0.2.0)\n",
            "  Downloading pycountry-20.7.3.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytablewriter==0.58.0 (from lm_eval==0.2.0)\n",
            "  Downloading pytablewriter-0.58.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting rouge-score==0.0.4 (from lm_eval==0.2.0)\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting sacrebleu==1.5.0 (from lm_eval==0.2.0)\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.2.0) (1.6.1)\n",
            "Collecting sqlitedict==1.6.0 (from lm_eval==0.2.0)\n",
            "  Downloading sqlitedict-1.6.0.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of lm-eval to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.11.0 (from lm-eval) (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.11.0\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.11.0+cu113 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.11.0+cu113\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command '# Remove previous installation if it exists\ncd /content\n# mkdir model_folder\npip uninstall -y lm-eval\n# rm -rf evaluation-pipeline/\n\n# Install evaluation-pipeline\n# cp -r /content/drive/MyDrive/VU Thesis/Code/BliMP_eval_code/evaluation-pipeline /content/\ncd evaluation-pipeline/\npip install -e \".[colab]\"\n# Install other necessary packages\npip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n\n# Unpack dataset\n# unzip filter_data.zip\n' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-655be3614e85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# Remove previous installation if it exists\\ncd /content\\n# mkdir model_folder\\npip uninstall -y lm-eval\\n# rm -rf evaluation-pipeline/\\n\\n# Install evaluation-pipeline\\n# cp -r /content/drive/MyDrive/VU Thesis/Code/BliMP_eval_code/evaluation-pipeline /content/\\ncd evaluation-pipeline/\\npip install -e \".[colab]\"\\n# Install other necessary packages\\npip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\\n\\n# Unpack dataset\\n# unzip filter_data.zip\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m       raise subprocess.CalledProcessError(\n\u001b[0m\u001b[1;32m    138\u001b[0m           \u001b[0mreturncode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       )\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '# Remove previous installation if it exists\ncd /content\n# mkdir model_folder\npip uninstall -y lm-eval\n# rm -rf evaluation-pipeline/\n\n# Install evaluation-pipeline\n# cp -r /content/drive/MyDrive/VU Thesis/Code/BliMP_eval_code/evaluation-pipeline /content/\ncd evaluation-pipeline/\npip install -e \".[colab]\"\n# Install other necessary packages\npip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n\n# Unpack dataset\n# unzip filter_data.zip\n' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "#@title Setup script { display-mode: \"form\" }\n",
        "#@markdown Run this cell to install the necessary packages (may take a few minutes).\n",
        "%%shell\n",
        "# Remove previous installation if it exists\n",
        "cd /content\n",
        "# mkdir model_folder\n",
        "pip uninstall -y lm-eval\n",
        "# rm -rf evaluation-pipeline/\n",
        "\n",
        "# Install evaluation-pipeline\n",
        "# cp -r /content/drive/MyDrive/VU Thesis/Code/BliMP_eval_code/evaluation-pipeline /content/\n",
        "cd evaluation-pipeline/\n",
        "pip install -e \".[colab]\"\n",
        "# Install other necessary packages\n",
        "pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "\n",
        "# Unpack dataset\n",
        "# unzip filter_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP_CtLXNbz0U",
        "outputId": "8ba009d7-526b-412f-d52f-96484c1eda57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoPl0Ubub1zD",
        "outputId": "6b6f6611-da04-43ed-fb82-27a943ed0d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttj2qGrIb3ak",
        "outputId": "a6e6a3d0-d615-4efd-9282-fb4cb758bf4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ca32c1984d9c1400570d2be8e056010eaca8349bf7075deb314d1c98827b9803\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9XAVOPX_b4xZ",
        "outputId": "5ecf9eaf-fd16-471d-a859-0f0f830d94ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lm_eval\n",
            "  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/50.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval) (1.5.2)\n",
            "Collecting evaluate (from lm_eval)\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval) (3.5.0)\n",
            "Collecting jsonlines (from lm_eval)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm_eval) (2.10.2)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval) (0.14.0)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm_eval)\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: rouge-score>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from lm_eval) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval) (1.6.1)\n",
            "Collecting sqlitedict (from lm_eval)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from lm_eval) (2.6.0+cu124)\n",
            "Collecting tqdm-multiprocess (from lm_eval)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval) (4.51.1)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm_eval) (0.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from lm_eval) (0.3.8)\n",
            "Collecting word2number (from lm_eval)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from lm_eval) (10.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval) (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->lm_eval) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval) (3.11.15)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval) (1.17.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval) (5.3.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->lm_eval)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->lm_eval) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1->lm_eval) (0.21.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines->lm_eval) (25.3.0)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm_eval) (75.2.0)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval)\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval)\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval)\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval)\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval)\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval)\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval) (1.19.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->lm_eval) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval) (8.1.8)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->lm_eval) (2025.2)\n",
            "Downloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Building wheels for collected packages: sqlitedict, word2number\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=177b1f43254b9b40aec9b46f4f264f9fe07ea7020b1692476fdd2cb353cf0be0\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=e0da035d3e2c98e3975a2a94eaaad778488e196448ab2a70d23ea3fba291d72a\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built sqlitedict word2number\n",
            "Installing collected packages: word2number, sqlitedict, tqdm-multiprocess, tcolorpy, pybind11, pathvalidate, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mbstrdecoder, jsonlines, typepy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, DataProperty, tabledata, evaluate, pytablewriter, lm_eval\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed DataProperty-1.1.0 evaluate-0.4.3 jsonlines-4.0.0 lm_eval-0.4.8 mbstrdecoder-1.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pathvalidate-3.2.3 pybind11-2.13.6 pytablewriter-1.2.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "558ebe231be0420da9e00b5e1e889fd4",
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install lm_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "51dd5520d1f34b7faadf3685c673c592",
            "1804d7bf509e4ca4a9c774a3322c57c2",
            "03c2b6a852864b28b59f7514bfe9b33a",
            "5582bdda513943fdb31c53917a0da230",
            "df0d53156c0b4cd3bbe84a538d232e55",
            "7f74b95acf25415e8e6935dc55b360a3",
            "389908f9042140c19f889ea1ef808dc5",
            "01f6f314d392445382c1cc1409e141ca",
            "f6a7b164dfb44ee2a74c2c88a572f1e7",
            "d7c566e95c6945339e479a7f92a51f01",
            "73ad884d99d34694a494215d82f6499a",
            "eae3fb6936e54838ba03bf0ecf43fbc3",
            "6177ec5d5a314bb99232e156c7b6d6aa",
            "99bfc8710f4e4235bb4c683fd9439174",
            "9bb0f2ddc9244783abea44d2514823e9",
            "7e3912f47c3a4032873f118270fe96c7",
            "a07e17fb2bc545f8b24afc05558f84b0",
            "ba9e88c5ddfa4464b260171c9875cd53",
            "1fe7599acef4422cb4d260e49cec5ea6",
            "78965a105897433f96fb72b65dc5e154",
            "7717b6a851eb43968208f74882aaa88d",
            "9c4a0e384a0b4ecc8687cdf2294083e7",
            "1684e260fc25457baed9a4f35b02c2e7",
            "262c038146064449b901672cf4de9b1c",
            "78a741af7add46f9bcf0f53c9c7c8f78",
            "612b73d48121487e9a338c1723d8d41b",
            "b512192f9ae249b4a18909f7fae440b0",
            "f77d88314bce478f9fcbf2bfae3fffaf",
            "fac818ac51c94cf7b8a4d73d51f99a22",
            "10ed5f3e97f44b8d8e5606c222145984",
            "825e660b2b644d2b9872d9447a564d44",
            "261d44db19dc455b9690a560a94cc4c3",
            "0a5211a16db0480e81c8ad7b8c2dc1c5",
            "09d6cec8af6e43f9ad9767559913a649",
            "fae09053b9814b93b424ddbd2a70191a",
            "001f571719864281b955647fef401909",
            "5692e202ea3048889c0ebb9586f3dbe2",
            "40a01b49b75c43c6b140a1d8d266de77",
            "bb9920c755a84b55a11eca72e27cc909",
            "9b5b6602ab2149168b918f8f51f45637",
            "c22f5ac391dd4453aadfc0b7c993f38e",
            "6e5b43f1eaa048fcbba3f8922b824ecb",
            "f434b783bcb74654bfe878717e734368",
            "23a9691c01184f8bb3ec6b6ea1a875ec",
            "7d1837eecd28450bb0ee9e859acacb6f",
            "7d1a672cd42d461d9f4240445c8444bf",
            "c5bdd3ffe283428fa94ab8b6c6290b97",
            "8932a69fd7904e8ba17d8c4c5a17bb13",
            "1bb645157ca34936824297cc3edf9c21",
            "a1f51c04009046bca46d94d6645c985e",
            "e9da223557a44788a20077f8c137e929",
            "b2f22a84e2df4c748d5bd9b417454cf8",
            "2756b4768cc74353a9f019df1f12fc6d",
            "bf7945bb6cad44819d5edb3792d86352",
            "721d572ba46d42dc92ea73468ab04fdd",
            "91dbaff3689c47308c694f8c27d9b2e5",
            "a3c37296615b4dc085b2b2469aff68f2",
            "8eee857c1e3b4dca943f73812e7e4d54",
            "7d4d5509c3c14e2182a32afdd8ef6c41",
            "998e169cc9f9478893a8691f75f4cfe7",
            "0a8b4081289c4660b410c9e4e25521a3",
            "7042d880f3be41ee90101eb7b8b89b21",
            "aa1d52e636354f1b8093d74517ec787c",
            "1b470692fb934448a1370c0954002605",
            "6e06c642194145f692604bae5c80a4b3",
            "9ccc453127964e97a93257949ed3564f",
            "468077879f6f442a93b4fbea9035aea6",
            "88f7dfff01f4469eb21c13d38d4973d9",
            "da62928204f4421f80c6d67ab358a2e4",
            "81b14a4828594b1b9c1d175574f4d634",
            "cb367adde2ac49da90e0915211629409",
            "6adedfa0199e460f873d8a8b4b253b0d",
            "1bfcc6b731e041d79a42e55e0b6dc30f",
            "55dcdeabde4142e0a2303546f2e21de2",
            "ff9c54f9c62c4af0a5343d754cb14d0f",
            "b9451bbf93554df4bc1cab2b1ba62f19",
            "7ae1c1e76e3247229d7e5404ba36b45f",
            "27eea9e05cb6417da0b5b60d518f8c1a",
            "e124a23efc4a4348a8111c3d469f5b16",
            "e49fe886f3be479fb6d41bb059c07c86",
            "60be244e65cf4e1b937708c8169a133a",
            "89571c30a795451d90e68f9762a1fe2a",
            "7737dacb43934c8cb3baa78285aac822",
            "22bd2836298b4cb5bf987738544ee689",
            "8f24439497fb4607aafff8a6c60ef754",
            "3153de56d25d44e2901b0e42a993e9c6",
            "d0f7c67cb0fb4fc18184bccdf201bf6a",
            "c840ab44160846798d45963f2fffdec0",
            "0d614cf2c5ac40ba9418090d97a2d412",
            "8510fb912a1b47cdbf2545bdf889f68e",
            "f2c44fe1bf1d4f2f90d16375f2fe2087",
            "9dd0efdda3ad4acc83e72488581fb01f",
            "2ae10dd76dc84a83b0e8f3c4c60573b2",
            "1184fa0b1d6c48318f39b033a1b4dedc",
            "3097b25af4da4cdb92a74a6c8dc7bf7a",
            "426cc33f6e4d421993c64690379a3adf",
            "413325042a7e40e3bdbbaeac19ed8d99",
            "b5ab6cc6914c48d6b83562d0997b7145",
            "6e76444c5e1e46de9e5da5876d98a0fe",
            "85cbd7d392df411db56a12d60b27be9c",
            "005cd854474b4bc3a11dd3db7b889a1e",
            "8292c37d3eba46c386401e2b89982a01",
            "e3dccaee8de94848821360a7a13acad2",
            "fd2fbe6cf8824fc69a163dafa6746917",
            "71275f8b3176442c8135a8a73368b755",
            "83380182df6840d3a4f55fbd56e23284",
            "879a9b57031e4e9da32d7364f24ff7a5",
            "ae97bb7ccf6e4e52a6082bddee442965",
            "000912d18eac4df1a216e6c728e8af0c",
            "dd508a7d233248f29d6c9866596dc8a5",
            "7f80dcde20e24b578d63af79fcc4f1ea",
            "c17e40547b1240c093bc86c51b98869d",
            "365f3ef8dba2420f9552efb719141afa",
            "2f3aa15bb9504e68b6ed3c4e3dcb048c",
            "5a514aea79594375865672d1a16dfc3a",
            "2052d06b2bc8437daa28166cc7851b1a",
            "ff50549b9bfe4ac79d520c6b9f22cd83",
            "c05a9422168b4489999826f955677291",
            "d5a49398822e45ff8ec055aa0b45f001",
            "a736cfd20e7f42c79d06add589edfd6f",
            "0e9e05944d9a4bb7858e9966dfa90b0f",
            "21e169bb9f244b7c833469ae284dae81",
            "101e7078022e44d9ab6be3a2e5b500ea",
            "d87cdebffb2c4b358bc9d973b8d0596b",
            "6caba7f1317740cba890d9ee86e198e9",
            "41735edee2d24545b4080e227b058c7a",
            "a2911bc4be634adca0ac09149112f513",
            "4088ec2030e040ca8b261f881eec7fd0",
            "bff7ed022ebc47ac9af59fc8e2b894f5",
            "e4fee367f35e4a838371ca201a895a4a",
            "5b148cc0a4ff4ca385d3b35a5f3d1d8e",
            "0a6fd7b0813849a9b45bf6310900c4a2",
            "253685c7b814447d8de3467941871062",
            "72ba831a1ec94087a2341a6b4efd0d93",
            "6144e71352194d4aa3892d4dbf9e7daf",
            "b84f35962fff402a90c0e04a9be6f529",
            "8d6aab6f352e4ae09c37e483169f5355",
            "920f64e6b4de4c77b152dffbff55b347",
            "40f514a8d0db46a8a114b9bcfe40af5e",
            "cb1bb15291394a9d8175a6d5b628cb1c",
            "161280e731a84c0a9c470f48622c6429",
            "e8356222e2474b7e8cdd8ecbf975d253",
            "3a465e88d49546b6a0ad251b45d8c25f",
            "2fce438e5f4b494d81e3084017c42843",
            "02afa47fb0e94089b1206809457325e9",
            "35325950062f48fcb784b86d195d2b37",
            "8b4b28a5119649cdb68b7940b43494be",
            "84693a1d164544ef88d6ce02cb90225d",
            "3fc4a41a30704428972796e27bfa7a26",
            "dc3ced64f27f4d628be4527ae5c2753c",
            "13cfe1f2488d4ae0bc646d46c4b192a6",
            "4a4754e7bd4944d2813aaae68f9edf74",
            "7d39aa259afe4cf1b771b34ec76db3b3",
            "f53b626d5e7c448f8e7a6a69107ac635",
            "692531ebe5dc4b8ba6b21a46fae23135",
            "bf755d6845164b3c9dfc9be255968d5d",
            "234ba324b89c4996bedceffb9ae65f7f",
            "c3951ce73a724c78ab80d40aa02d2dda",
            "39ca4abd7ca141baa592222ad2083c05",
            "3ebc2e66ecd643e0b50c3f1ed62f005d",
            "ec4bb1dde2554c1ab6f50e44e293a3d5",
            "74ec7a6211a54ab49a44c3360456845c",
            "73e19ba9609e4e03930d05b88a450477",
            "0b72928d0c4f44538b12404f4657aa8f",
            "1dd8a2301be5443da4f721d18d55969a",
            "03ba10feb86348f2869803617538cb9b",
            "8f1f0ef638274f438691f1c5556eb4e0",
            "c20530db7edd4a4d9c700cff6c346c5c",
            "115692bc8ae446b1abf0e01cf41a8a5d",
            "49df3d376cc541b4a56dc67d25e01492",
            "bdd24a99d3c74f1ab3bb9c2bb22b0790",
            "3a1e6e01b6d345e78e6ed27ca46a2d35",
            "66e32ca486f7461c90c63cdb2ef2067d",
            "46acb934dd454f53917eb8d0c748f81a",
            "eb3c7cb17bb047a590bbb97c8ff3e36a",
            "b328aaa7f07649cea4f7670d99bfb3ab",
            "6f6ac693dab946a0a5429360020fb5f5",
            "40636a7bad75435d9575f7a6a990506d",
            "9ab982c359864557823dc672ddcec615",
            "2cfe07211e9f4f81b6cabfa07cb81fdc",
            "694bb51d76d74cf1a9901167d6b71967",
            "e60e7e6f32264047b978276d97ed2c70",
            "d7d23c02a7f1494c9c33aa220e3d1797",
            "58f2d5cbcdf04a43b4c1da65234091fe",
            "a0f560a35e5b4cff87aa8b80c5b45629",
            "c5b7fab6d47c48d98200fb6c496ef627",
            "1c86feca2f0c4edfb357eb5eae6d0f81",
            "048b6ad0da934bd196c12dca5ba348bf",
            "d228e0bb92d044fe9b6afaebf1345bac",
            "d31ebb7acf054a1781c7680a6dae9c46",
            "7b54d2c6b1674d748aa59f2b6684cf2b",
            "240529da408449ee96787ec63d0f94e5",
            "99c826163bff4604b6eac5b163c0f471",
            "b2201f1a7e514386940410396df53cbb",
            "f68e8fc8cc78440f87e40fac788edab9",
            "ab38eb0d14854f8699a3e1cc71da6ab8",
            "3f766231bf014c9eaffbd26fb1be3ede",
            "1f1a110134cb43ff92f42169ae74fc9d",
            "3ed1053e00cf4a02b1f04f13597e4ea6",
            "7e7b4d487410413f8f3e233b0910d049",
            "a7bb12bb68764260a79029a55e1cc303",
            "3f4dfbdad2c04f37a877d02e944b8062",
            "d2652756fef644cba14801dc7ba5e4f0",
            "9b98849f36dd417db7cc1207f559593d",
            "909c18b67b234eddbcf8fb55919bda42",
            "591ad8f15eaa4665b507a5afc14d0c5a",
            "a0cf68e2cd3b4b589a20b7a2577e4984",
            "a237e8c9aa5b4db8a60f304d3cc783c2",
            "66a79874f49a45d8aadd22df52b7db42",
            "4854c3e33fe643afa98d88bb7bb2acdc",
            "d2c317dd53834166855be5938993e1c1",
            "c74235542e5645a6b0039f4394fb2766",
            "7e11ed2d348b42f381ad83f34593081b",
            "fea79d7d86084503b5c17cea4e8fa7f6",
            "e85a65e9d5824688ab3cc805fc5b5728",
            "7d43650269ef4ff5904f707167fcd60a",
            "6ba49e0cf5c0437daf9759a2b1ba3c4f",
            "29e4917ad3d74929942de12e68f4aded",
            "5e4aeecf8491423bb068f96b29b3c742",
            "c775551e30934ac891b307b1a2a6688d",
            "4b2282b704644c3ab441e8ca8c9b21e8",
            "358f468da71a4bee92b777270256d9d1",
            "08407aa083144497aea008a6a6fa16cc",
            "ee9baea995d84b46a37526eb179b8768",
            "30d8454e67d84edb8a84795bfbfd0485",
            "53cc1f7658114f8db10005e0f59883f9",
            "768dca10c391432c9a3023f20f76b319",
            "a7eb7587ffbb481b81af743299857835",
            "fdf0cc397ba94fb2900f4252b14afc21",
            "baf4e1becc124f2d8a8f8e2fc2d94762",
            "44ad448f50984f00b8ae59b68a63113e",
            "75b8e57e429b4bc59fffe0f79c30a96b",
            "2320c9bdedaa47b5999dd6f6a655a3cb",
            "491b1192854b48a787964f7544d11095",
            "345547909cc74fa28388cfd71a9b642a",
            "6356d0b5d63d46c59d61d3fb96291d85",
            "e9138e26c7ea44d29431058f5f88e91b",
            "6707c4edc8df484bb12979868cdaf1b5",
            "7d87bfc652ab47629ce01ef5bb4871e2",
            "a64dfd54b6aa4346b29cca9ca813a5be",
            "c41690a5aa304d5682357717e5e07a71",
            "d20a48f2ca4c4d55b9673bfe3e305a95",
            "12f89ef72f7449f7bec880b132f2f2d8",
            "cc33ae3111704ed9b5806723eef9848c",
            "13d9bd2cf1924846b4693095bab66cef",
            "0f00d06ca52d4cd09a14b7ee57013a98",
            "c127b2b55ac84bd382c9d7768b063db3",
            "9acd3ad8e2194c1bbf7360b24d658133",
            "b390659de6b3407492ff95b455a9a25a",
            "3128d726cee148a4bac92a4f81604f4d",
            "8e2febbebcf3464a898fcfa00416c049",
            "a2d8ff0348e84140b7d7b4c7075a0cde",
            "de3da6a33498492da1446df8db01c156",
            "787d6d5799b64803994adf83d368b177",
            "166ba02ce7f34f928ce473c02e023e72",
            "4397fc78d1874be097383790bda0c312",
            "2068b20959a642109f36c16b9bec2fa5",
            "cb883ac932de487aacf8bf74da130b23",
            "c68a01fa97754290b0e22dd3032e7093",
            "8e797d52a2cc4e7a927504258396642c",
            "47752964cb3a4421911aa9054bc23f69",
            "110668e55c8c4116aee4cb774321cd83",
            "cfb707d7a98445429292ee39001829e7",
            "c74c9f5bbbb14c01a51ca7c449faf284",
            "d3a12b88fd144c0bb0a66033c7641b3d",
            "c374ae4e97be4c56b380cedefa92df68",
            "3e026ff07a994128a87537ed3560bb6d",
            "3b21dbbf469a4ceaa2a86e93792c43f1",
            "d5f0372d375149a094fbe848d5c0764e",
            "3abec07a01f240cab241f72a9ea91144",
            "85bfeb4f32544975a0971cc0ca61e66d",
            "c1e462a1072b4cacae4eaca558a8c99b",
            "ee69fdd06d2d4299b6757d780c839a3f",
            "846d2719a74840e8a45ca78e5d978c9d",
            "0e89d2491c3244fcbfb484b260d0e22b",
            "74bade329a6148d1be917676d65df77f",
            "c14f643460dd445a9a63e5eda6009ad9",
            "33aa0f4596b14615b0e60250b51bccb0",
            "ead3f3cfdc0840fca76577fe28fc6063",
            "ffc285900a1c448d8a237fc6e4301e9e",
            "b7b7041307cb438e92165a6691005cc5",
            "1ac71bf3f48a472a9942b17c89104ffc",
            "f4aa7a7d90174e18ae94557064a8a13f",
            "589d0e753f194562aa6275ef2a82c024",
            "267ab94a8214430ba5029f5d6bedf42d",
            "7ca36cf0c7c547d58271b39bae45bec0",
            "40838df70aff48bd8b8b65e8214d5902",
            "77f511d3733140f69384a2be1949c658",
            "43e23c4279cf46bb9b71f107d7507c4c",
            "5726109cb2fd409e9ab293bdd2cd0a67",
            "f5e5b26a66004fa69a91a3dcf359cc0f",
            "b297eefb46694eecb033d047641d0b72",
            "061023b9a7bd4cf2ac8ba91261344d35",
            "ac295dc7046942dc8ae2d19439f3da62",
            "e0f66f4a73a44d33a2e3ff7006834d73",
            "63004d9b81684c3bba8f17f5439ca85d",
            "980dfce18a4d4edbbfd536262def0dd1",
            "516e8a77188d4e84b91ae5c64815d7e0",
            "525240161f144bb9807e180c7468d36c",
            "6cc9ab5d53544aae80a2d840a50de5fd",
            "da364c2580724551aeca9aa780ce7ac5",
            "c2d3d0cda77d4f23877927e3c1953533",
            "e1bf67bdd35840499c0dcb4b45b370b8",
            "b964b4ef7ce342fd9caa847b1c045ab4",
            "642b529468dc474c9dd5f1b4e80c01bb",
            "a49fa3c6ddc9446db4525f480002ec20",
            "76bae650b5124d7ca3e78f851987af9e",
            "4fb5ba35a89c409d8e155f6f56dcb632",
            "2a20f47f6e034c95a96097f847ed751c",
            "c7224f086f20471daecb7b657c8fa2ee",
            "99712d45e7684542831d5695006c467b",
            "e4110fd3ec2c486eb6bbdc9e61fdc173",
            "d444057c10fe4bafafc5b5c1d4c71d0e",
            "0280ffa9bf28483dbab7ecaf323ec5c1",
            "cdeab3caac0740bab27e84333c86cb7d",
            "98addb92bee946ac80a981b69dfa4e59",
            "3206b9fd09f04a78881738e5d98216b0",
            "13997c5af8d34e29ad34df9fa8ab0f51",
            "99b1d7851d324fe9a5592ec8e8d351ba",
            "049e06e80bf74fcfb4feec65abaa4c9d",
            "a8ff59031ae84a1d9978a12e4a8c598a",
            "b7583fb9025643f7abdce1e5236b3bff",
            "6da58917f1604ea281de2b95c14fdd00",
            "b896d097904849f880fd1a6d99356fbe",
            "0d3579ba48914ba68b06c56589d1e8bf",
            "27ee6d0855fc4475bad20ea9c38bd2ff",
            "e26e8a0372614a4d93c2e98d71b31361",
            "3ca28191d84341c6adbc0831dcf862d4",
            "f7ccbfafd0594123a45bbf9210884f7c",
            "7bbe342bb63d428a81bf203704c5b9e0",
            "0500834d09844ccbb3cfdc9bc5d147b8",
            "824c3160a9ad431cbcfe92dd1a3badc7",
            "a948c727014947bfbf6bc5bd9cf588a3",
            "f35e86f91f2a4056a298f6c05358a165",
            "201de57f05064e18a635818dfd718277",
            "863a05925813454eba5711a95c67469b",
            "4c3f93e631314542886ce288f788f0f2",
            "fe256823fff94612ae06ee55f4f9e6ab",
            "79d26953bc5442b3801f14e4911f145c",
            "52d3d3914fcf46c5994b7f1d21788441",
            "40c8ab47254f41aabefbc4cc37d78ba2",
            "ab51188488d146c9978fc7ade6d0a9dc",
            "f218deb46614419788460bef6d87a051",
            "26df3c39ccae48048af4b8309066ae39",
            "789d2575572548e2b95b00d2246827ee",
            "783b933d566542589eb1d816d543bbad",
            "49f9129c98594e3283552c0fca2e8f2c",
            "48c6f47a08f64b4987feaf462e424435",
            "f6ded4a2c0014e2694b92612b6d8cb0f",
            "9c7ff6c950f94ee0a542af62829efd01",
            "e7ce506a2a164e3c883314d75dbe2800",
            "e69669f7e20e4dd9bb640eaa983ef6ef",
            "8da1784ae457443f96f2dd8327657e05",
            "71cea5903f8d4126a9bd4102eeaa8609",
            "ff4edd9416864425b2c7d22665402ab6",
            "24026191252148f99d73be0c77271e7f",
            "7e2c38e399364007b6b5c0a9058e2daa",
            "f18ca20d13a543e1aa916795ae14e8ba",
            "bb19f7dd43484b8fb87545d818dafa7d",
            "3bef98a01b4f4034bdf84640340f3c32",
            "28426daec5be47b49a8ddd254b6291d7",
            "f1593ae6d24a46eba8ab6cafc6ab0b7d",
            "53504284358d4151a07726e4739ed8f3",
            "a5992a0c7ba84f08a65ae33c569bc01c",
            "123f14aaee77473fa04f9efdf1a8321f",
            "18823d716ce245789e7d00cd64a785de",
            "82738590945844f485e8f5b7671a2d0c",
            "19c712b5862c4dc88def0141f8b55b83",
            "f1d68b60a91842f7af031878e8dffbef",
            "bd3fff651880435f9d77014536b329d5",
            "dde6652d563e423fbbd1942f23ffe834",
            "567195b5fadb4a40967a80902c152340",
            "37f5ca57125940faa4a075f80af70ed0",
            "0bbef61de1b142a4808aa1afff47b78f",
            "e477f5efa7e14feaa1feb47f04478ed6",
            "bc1ac39522bf4a8198395bd5ecab81d7",
            "d12a24e24f134ec1b3994a603cc3fc97",
            "196e869e76e743aaa1a11ce20dc41547",
            "09013846468546f79db8b38b8022f6dd",
            "b469fa08200642bf8f8b31f5435390a3",
            "9eac8a50ca784f76a34f9cc05335d1d4",
            "a5b8a3f9436b4397a07e3724cd41038e",
            "8dd2878e3d5840f5aa92ef653d527754",
            "b159e184310d4780a914e5a6cbf94235",
            "47176d4d5ebf4800a8fffa2b4c0ec897",
            "409cd4145b374c1c84138cecaf401a13",
            "59dc79696f734215831727e3f07c8a7b",
            "4c31722fbf1b412fb1db67cb00be627e",
            "9ff8fcf7367045389ee62e3f23f5a6ac",
            "dc7dd7b7542c4d66b973e1b70d676b94",
            "3329aed17a2f4ff8bd9ad0b6c2a8d3ae",
            "3929a558b66142f9addd38b9b8423c3e",
            "440c1250caf24f25a51ba95de8158514",
            "9ebeb5695a5a4e5f821d9ff96b052d98",
            "7811c1719ee9441b92cd5373f705ec8d",
            "c39edaaa191645b79ff98d7b94f8a791"
          ]
        },
        "id": "ZkLlbEO8b9SY",
        "outputId": "3e4000f3-af08-40f9-c9cd-d3cf0dfc1b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/evaluation-pipeline\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51dd5520d1f34b7faadf3685c673c592",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eae3fb6936e54838ba03bf0ecf43fbc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1956 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1684e260fc25457baed9a4f35b02c2e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1956 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 1956/1956 [00:00<00:00, 7248.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 3912/3912 [00:22<00:00, 171.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anaphor_agreement:\t64.98%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09d6cec8af6e43f9ad9767559913a649",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d1837eecd28450bb0ee9e859acacb6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8248 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91dbaff3689c47308c694f8c27d9b2e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/8248 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 8248/8248 [00:01<00:00, 5390.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 16496/16496 [01:33<00:00, 176.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argument_structure:\t62.43%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "468077879f6f442a93b4fbea9035aea6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27eea9e05cb6417da0b5b60d518f8c1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6738 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d614cf2c5ac40ba9418090d97a2d412",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/6738 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 6738/6738 [00:01<00:00, 6511.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 13476/13476 [01:19<00:00, 170.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binding:\t61.59%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85cbd7d392df411db56a12d60b27be9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f80dcde20e24b578d63af79fcc4f1ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21e169bb9f244b7c833469ae284dae81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/4526 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 4526/4526 [00:00<00:00, 7132.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 9052/9052 [00:55<00:00, 164.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "control_raising:\t59.77%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "253685c7b814447d8de3467941871062",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fce438e5f4b494d81e3084017c42843",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7542 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "692531ebe5dc4b8ba6b21a46fae23135",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/7542 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 7542/7542 [00:01<00:00, 5360.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 15084/15084 [01:25<00:00, 176.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "determiner_noun_agreement:\t77.29%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03ba10feb86348f2869803617538cb9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f6ac693dab946a0a5429360020fb5f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1732 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "048b6ad0da934bd196c12dca5ba348bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1732 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 1732/1732 [00:00<00:00, 6774.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 3464/3464 [00:24<00:00, 144.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ellipsis:\t52.66%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ed1053e00cf4a02b1f04f13597e4ea6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4854c3e33fe643afa98d88bb7bb2acdc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6426 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b2282b704644c3ab441e8ca8c9b21e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/6426 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 6426/6426 [00:00<00:00, 7082.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 12852/12852 [01:20<00:00, 159.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filler_gap:\t60.26%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75b8e57e429b4bc59fffe0f79c30a96b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12f89ef72f7449f7bec880b132f2f2d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1965 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "787d6d5799b64803994adf83d368b177",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1965 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 1965/1965 [00:00<00:00, 7148.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 3930/3930 [00:21<00:00, 179.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "irregular_forms:\t84.68%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3a12b88fd144c0bb0a66033c7641b3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74bade329a6148d1be917676d65df77f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2676 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40838df70aff48bd8b8b65e8214d5902",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/2676 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 2676/2676 [00:00<00:00, 6841.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 5352/5352 [00:33<00:00, 161.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "island_effects:\t44.51%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "516e8a77188d4e84b91ae5c64815d7e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a20f47f6e034c95a96097f847ed751c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6586 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "049e06e80bf74fcfb4feec65abaa4c9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/6586 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 6586/6586 [00:01<00:00, 5033.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 13172/13172 [01:17<00:00, 168.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "npi_licensing:\t59.52%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0500834d09844ccbb3cfdc9bc5d147b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab51188488d146c9978fc7ade6d0a9dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8da1784ae457443f96f2dd8327657e05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/3882 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 3882/3882 [00:00<00:00, 7181.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 7764/7764 [00:44<00:00, 173.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "quantifiers:\t67.62%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5992a0c7ba84f08a65ae33c569bc01c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Assigning unique IDs to 'blimp_from_file+null' docs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e477f5efa7e14feaa1feb47f04478ed6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5535 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Filtering invalid docs from 'blimp_from_file+null'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "409cd4145b374c1c84138cecaf401a13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/5535 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lm_eval.evaluator:\n",
            "禄 Constructing 'blimp_from_file+null' contexts and requests\n",
            "100%|| 5535/5535 [00:01<00:00, 4525.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "禄 Running all `loglikelihood` requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:lm_eval.evaluator:\n",
            "禄 Running all `loglikelihood` requests\n",
            "100%|| 11070/11070 [01:02<00:00, 176.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "subject_verb_agreement:\t55.01%\n",
            "\n",
            "Scores:\n",
            "anaphor_agreement:\t64.98%\n",
            "argument_structure:\t62.43%\n",
            "binding:\t61.59%\n",
            "control_raising:\t59.77%\n",
            "determiner_noun_agreement:\t77.29%\n",
            "ellipsis:\t52.66%\n",
            "filler_gap:\t60.26%\n",
            "irregular_forms:\t84.68%\n",
            "island_effects:\t44.51%\n",
            "npi_licensing:\t59.52%\n",
            "quantifiers:\t67.62%\n",
            "subject_verb_agreement:\t55.01%\n"
          ]
        }
      ],
      "source": [
        "#@title Load model and evaluate (BLiMP) { display-mode: \"form\" }\n",
        "model = \"/content/model_folder/\" #@param {\"type\": \"string\"}\n",
        "model_type = \"encoder\" #@param [\"decoder\", \"encoder\", \"encoder-decoder\"]\n",
        "# file_name = \"examples3.csv\" #@param {\"type\": \"string\"}\n",
        "# model_names = [\"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\"] #@param {\"type\": \"raw\"}\n",
        "\n",
        "%cd /content/evaluation-pipeline\n",
        "%run /content/evaluation-pipeline/babylm_eval.py \\\n",
        "  \"$model\" \\\n",
        "  \"$model_type\" \\\n",
        "  -t \"blimp\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUY6rGfRdHth"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/model_folder/zeroshot\" \"/content/drive/MyDrive/VU Thesis/Code/baby_models/V20L4/model_e10_v20_l4\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
